


























import pandas as pd
import numpy as np
import nltk
nltk.download('punkt') # download the NLTK datasets
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
import plotly as py
import math
import helpers.data_mining_helpers as dmh
from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px


get_ipython().run_line_magic("matplotlib", " inline")


# load data
new_data_set_file = "./newdataset/Reddit-stock-sentiment.csv"
df = pd.read_csv(new_data_set_file)
print(df.columns)
# explore columns and keep the ones we are interested in
df = df[["text", "label", "subreddit"]]








print(df.sample(n=10))
print("\n")

i = 1
linebreaker = "="*30
for text in df.head(n=5)["text"]:
    print(f"{linebreaker} Begin number: {i} {linebreaker}")
    print(f"{text}")
    print(f"{linebreaker} End number: {i} {linebreaker} \n")
    i=i+1






print(type(df))
print(df.columns)
print(df.head(n=10)["text"])
print(df.head(n=10)["label"])
print(df.tail(n=10)["subreddit"])






# Lets look at wallstreetbets
df[df["subreddit"]=="wallstreetbets"][::10].head(n=5)






# Check for empty elements

dups_df = df[df.duplicated()==True]
print(dups_df)
print(len(dups_df))

df.isnull().apply(lambda x: dmh.check_missing_values(x))





df.isnull().apply(lambda x: dmh.check_missing_values(x), axis=1)











# Seems like we do not have missing value in our columns but we have duplicates
# lets drop those

clean_df = df.drop_duplicates()
print(clean_df)
df = clean_df








df_sample = df.sample(n=250)
print(len(df))
print(len(df_sample))
print(df)
print(f"\n {df_sample}")





# Since this is a smaller dataset, maybe we dont need sampling
# But we can sample for fun and see if it has the same label distribution
# Also we can check both subreddit and label distribution

df["label"].value_counts().plot(kind = 'bar',
                                           title = 'Label distribution',
                                           ylim = [0, 800], 
                                           rot = 0, fontsize = 12, figsize = (8,3))




df["subreddit"].value_counts().plot(kind = 'bar',
                                           title = 'Subreddit distribution',
                                           ylim = [0, 800], 
                                           rot = 0, fontsize = 12, figsize = (8,3))


df_sample = df.sample(n=250)

df_sample["label"].value_counts().plot(kind = 'bar',
                                           title = 'Label distribution',
                                           ylim = [0, 250], 
                                           rot = 0, fontsize = 12, figsize = (8,3))




df_sample["subreddit"].value_counts().plot(kind = 'bar',
                                           title = 'Subreddit distribution',
                                           ylim = [0, 250], 
                                           rot = 0, fontsize = 12, figsize = (8,3))





max_count = df["subreddit"].value_counts().max()
pd.concat([df["subreddit"].value_counts(), df_sample["subreddit"].value_counts()], axis=1).plot(kind = 'bar',
                                           title = 'Subreddit distribution',
                                           ylim = [0, max_count+max_count*0.1], 
                                           rot = 0, fontsize = 12, figsize = (8,3))





# Download stop words
from nltk.corpus import stopwords
nltk.download("stopwords")
stop_words = set(stopwords.words("english"))





count_vect = CountVectorizer()
df_counts = count_vect.fit_transform(clean_df["text"])
clean_count_vect = CountVectorizer(stop_words='english')
print(type(df_counts))



dt_df = pd.DataFrame(df_counts.toarray(), index=df.index) 
# we want to preserve index even after dropping duplicates
stop_words = set(stopwords.words("english"))
feature_names = count_vect.get_feature_names_out()
droplist = [i for i in dt_df.columns if feature_names[i] in stop_words]
cleaned_dt_df = dt_df.drop(droplist, axis=1)
cleaned_df_counts = clean_count_vect.fit_transform(df["text"])

print(cleaned_dt_df)










analyze = count_vect.build_analyzer()
analyze(df["text"][0])








# we use plotly instead here

term_frequencies = np.asarray(cleaned_df_counts.sum(axis=0))[0]
tmp_df = pd.DataFrame({"term": clean_count_vect.get_feature_names_out()[:300],"frequency" :term_frequencies[:300] })
smaller_df = tmp_df[tmp_df["frequency"]>2]
# print(smaller_df)
fig = px.bar(smaller_df, x="term", y="frequency")
fig.show()








sorted_df = smaller_df.sort_values(by="frequency", ascending=False)
fig = px.bar(sorted_df, x="term", y="frequency")
fig.show()





term_frequencies_log = [math.log(i) for i in term_frequencies]
tmp_df = pd.DataFrame({"term": clean_count_vect.get_feature_names_out()[:300],"frequency" :term_frequencies_log[:300] })
df_smaller = tmp_df[tmp_df["frequency"] > 1] # change threshhold when using log
df_sorted = df_smaller.sort_values(by="frequency", ascending=False)
fig = px.bar(df_sorted, x="term", y="frequency")
fig.show()











top_20_doc = dt_df.astype(bool).sum(axis=1).nlargest(n=20)
top_20_word = dt_df.sum(axis=0).nlargest(n=20)

plot_x = ["term_" + str(i) for i in count_vect.get_feature_names_out()[top_20_word.index]]
plot_y = ["doc_" + str(i) for i in top_20_doc.index]
plot_z = df_counts[top_20_doc.index][:, top_20_word.index].toarray()

df_todraw = pd.DataFrame(plot_z, columns = plot_x, index = plot_y)
plt.subplots(figsize=(20, 10))
ax = sns.heatmap(df_todraw,
                 cmap="PuRd",
                 fmt="g",
                 vmin=0, vmax=20, annot=True)





top_20_doc = cleaned_dt_df.astype(bool).sum(axis=1).nlargest(n=20)
top_20_word = cleaned_dt_df.sum(axis=0).nlargest(n=20)

plot_x = ["term_" + str(i) for i in count_vect.get_feature_names_out()[top_20_word.index]]
plot_y = ["doc_" + str(i) for i in top_20_doc.index]
plot_z = df_counts[top_20_doc.index][:, top_20_word.index].toarray() # use old index df_counts since
# cleaned_dt_df kept the same index from previous.

df_todraw = pd.DataFrame(plot_z, columns = plot_x, index = plot_y)
plt.subplots(figsize=(20, 10))
ax = sns.heatmap(df_todraw,
                 cmap="PuRd",
                 fmt="g",
                 vmin=0, vmax=5, annot=True)









# not interested in stop words
labels = clean_df["label"]

# regardless how many times the term occurs in the document
# if the document has score -1 then that adds -1 to that term
binary_cleaned_dt_df = cleaned_dt_df.astype(bool)

term_score = binary_cleaned_dt_df.multiply(labels, axis=0).sum(axis=0)


top_20 = term_score.sort_values().tail(n=20)
top_20_df = pd.DataFrame({"term": count_vect.get_feature_names_out()[top_20.index],"term_score" :top_20 })
fig = px.bar(top_20_df, x="term", y="term_score")
fig.show()
# print(term_score)





last_20 = term_score.sort_values().head(n=20)

last_20_df = pd.DataFrame({"term": count_vect.get_feature_names_out()[last_20.index],"term_score" :last_20 })
fig = px.bar(last_20_df, x="term", y="term_score")
fig.show()





### Begin Assignment Here
