


























import pandas as pd
import numpy as np
import nltk
nltk.download('punkt') # download the NLTK datasets
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
import plotly as py
import math
import helpers.data_mining_helpers as dmh
from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px


get_ipython().run_line_magic("matplotlib", " inline")


# load data
new_data_set_file = "./newdataset/Reddit-stock-sentiment.csv"
df = pd.read_csv(new_data_set_file)[["text", "label"]]





print(df.sample(n=10))
print("\n")

i = 1
linebreaker = "="*30
for text in df.head(n=5)["text"]:
    print(f"{linebreaker} Begin number: {i} {linebreaker}")
    print(f"{text}")
    print(f"{linebreaker} End number: {i} {linebreaker} \n")
    i=i+1



# Check for empty elements

# df[df.isnull()==False]
dups_df = df[df.duplicated()==True]
print(dups_df)
print(len(dups_df))

df.isnull().apply(lambda x: dmh.check_missing_values(x))

#df[df.notna()[["text", "label"]] == False]





# Seems like we do not have missing value in our columns but we have duplicates
# lets drop those

clean_df = df.drop_duplicates()
print(clean_df)





# Since this is a smaller dataset, maybe we dont need sampling
# But we can sample for fun and see if it has the same label distribution

df["label"].value_counts().plot(kind = 'bar',
                                           title = 'Label distribution',
                                           ylim = [0, 800], 
                                           rot = 0, fontsize = 12, figsize = (8,3))



df_sample = df.sample(n=100)

df_sample["label"].value_counts().plot(kind = 'bar',
                                           title = 'Label distribution',
                                           ylim = [0, 100], 
                                           rot = 0, fontsize = 12, figsize = (8,3))







# Download stop words
from nltk.corpus import stopwords
nltk.download("stopwords")





count_vect = CountVectorizer()
df_counts = count_vect.fit_transform(clean_df["text"])
print(type(df_counts))






dt_df = pd.DataFrame(df_counts.toarray(), index=clean_df.index) 
# we want to preserve index even after dropping duplicates
stop_words = set(stopwords.words("english"))
feature_names = count_vect.get_feature_names_out()
droplist = [i for i in dt_df.columns if feature_names[i] in stop_words]
cleaned_dt_df = dt_df.drop(droplist, axis=1)


print(cleaned_dt_df)







# we use plotly instead here

term_frequencies = np.asarray(df_counts.sum(axis=0))[0]
df = pd.DataFrame({"term": count_vect.get_feature_names_out()[:300],"frequency" :term_frequencies[:300] })
smaller_df = df[df["frequency"]>2]
print(smaller_df)
fig = px.bar(smaller_df, x="term", y="frequency")
fig.show()





sorted_df = smaller_df.sort_values(by="frequency", ascending=False)
fig = px.bar(sorted_df, x="term", y="frequency")
fig.show()











top_20_doc = dt_df.astype(bool).sum(axis=1).nlargest(n=20)
top_20_word = dt_df.sum(axis=0).nlargest(n=20)

plot_x = ["term_" + str(i) for i in count_vect.get_feature_names_out()[top_20_word.index]]
plot_y = ["doc_" + str(i) for i in top_20_doc.index]
plot_z = df_counts[top_20_doc.index][:, top_20_word.index].toarray()

df_todraw = pd.DataFrame(plot_z, columns = plot_x, index = plot_y)
plt.subplots(figsize=(20, 10))
ax = sns.heatmap(df_todraw,
                 cmap="PuRd",
                 fmt="g",
                 vmin=0, vmax=20, annot=True)





top_20_doc = cleaned_dt_df.astype(bool).sum(axis=1).nlargest(n=20)
top_20_word = cleaned_dt_df.sum(axis=0).nlargest(n=20)

plot_x = ["term_" + str(i) for i in count_vect.get_feature_names_out()[top_20_word.index]]
plot_y = ["doc_" + str(i) for i in top_20_doc.index]
plot_z = df_counts[top_20_doc.index][:, top_20_word.index].toarray()

df_todraw = pd.DataFrame(plot_z, columns = plot_x, index = plot_y)
plt.subplots(figsize=(20, 10))
ax = sns.heatmap(df_todraw,
                 cmap="PuRd",
                 fmt="g",
                 vmin=0, vmax=5, annot=True)









# not interested in stop words
labels = clean_df.loc[cleaned_dt_df.index, 'label']

# regardless how many times the term occurs in the document
# if the document has score -1 then that adds -1 to that term
binary_cleaned_dt_df = cleaned_dt_df.astype(bool)

term_score = binary_cleaned_dt_df.multiply(labels, axis=0).sum(axis=0)


top_20 = term_score.sort_values().tail(n=20)
top_20_df = pd.DataFrame({"term": count_vect.get_feature_names_out()[top_20.index],"term_score" :top_20 })
fig = px.bar(top_20_df, x="term", y="term_score")
fig.show()
# print(term_score)





last_20 = term_score.sort_values().head(n=20)

last_20_df = pd.DataFrame({"term": count_vect.get_feature_names_out()[last_20.index],"term_score" :last_20 })
fig = px.bar(last_20_df, x="term", y="term_score")
fig.show()





### Begin Assignment Here
